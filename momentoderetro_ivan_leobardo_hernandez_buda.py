# -*- coding: utf-8 -*-
"""MomentoDeRetro_Ivan Leobardo Hernandez Buda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e5fAgqMFAMTDrcOq1-Z861QQR71L8TvG
"""

# READING DATABASE

'''
En este código se implementa una red neuronal de 2 entradas, 1 salida, y una dimensión de capa oculta de 128.

El 'learning rate' (alpha) define la cantidad de corrección que se le aplica a los pesos y bias al actualizarse.
Sin embargo, el 'learning rate' puede únicamente modificarse entre 0.00005 y 0.00003, ya que la red neuronal se
vuelve muy sensible a este cambio y la pérdida 'loss' no baja de cuatro órdenes magnitud.

También, la red neuronal no puede bajar de una dimensión de capa oculta menor a 128, ya que también se vuelve
sensible y la péridad final se incrementa.
'''

import numpy as np
import matplotlib.pyplot as plt

# Extraemos los datos del archivo el cual contiene 'género', 'altura', 'peso' y 'bmi'
# Al extraer los datos, por a la falta de librería pandas, se pierden los datos de 'género'
data = np.genfromtxt('bmi.csv', delimiter=',', skip_header=1)
height = data[:, 1]
weight = data[:, 2]
bmi = data[:, 3]

# 'X' serán las features (que corresponden a los valores de entrada de height y weight)
# 'y' será la salida (que corresponde a los valores de Body Mass Index)
X = np.column_stack([height, weight])
y = bmi.reshape((-1, 1))

print(data) # (500, 1)

# NEURAL NETWORK MODEL

def relu(x):
    return np.maximum(0, x) # Non-linear activation function

def forwardPropagation(X, W_k, b_k, W_m, b_m):
  hidden_input = np.dot(X, W_k) + b_k
  hidden_output = relu(hidden_input)
  y_model = np.dot(hidden_output, W_m) + b_m
  return y_model, hidden_input, hidden_output

def backPropagation(X, W_m, hidden_input, hidden_output, grad_output):
  dW_m = np.dot(hidden_output.T, grad_output)
  db_m = np.sum(grad_output, axis=0)
  grad_hidden = np.dot(grad_output, W_m.T)
  grad_hidden[hidden_input <= 0] = 0
  dW_k = np.dot(X.T, grad_hidden)
  db_k = np.sum(grad_hidden, axis=0)
  return dW_k, db_k, dW_m, db_m

def updateWeights(W_k, b_k, W_m, b_m, dW_k, db_k, dW_m, db_m, alpha):
  W_k -= alpha*dW_k
  b_k -= alpha*db_k
  W_m -= alpha*dW_m
  b_m -= alpha*db_m
  return W_k, b_k, W_m, b_m

def getLoss(y, y_model):
  return np.mean((y_model - y) ** 2) # Mean-Squared Error

def trainModel(X, y, alpha, epochs):
  # Hiper-parameters setup
  inputDim = 2
  hiddenDim = 128
  outputDim = 1
  N = len(y)
  loss = []

  # Initialize weights and biases
  np.random.seed(0)
  W_k = np.random.randn(inputDim, hiddenDim)
  b_k = np.zeros((1, hiddenDim))
  W_m = np.random.randn(hiddenDim, outputDim)
  b_m = np.zeros((1, outputDim))

  for epoch in range(epochs):
    # Forward propagation
    y_model, hidden_input, hidden_output = forwardPropagation(X, W_k, b_k, W_m, b_m)

    # Backpropagation
    grad_output = 2*(y_model - y)/N
    dW_k, db_k, dW_m, db_m = backPropagation(X, W_m, hidden_input, hidden_output, grad_output)

    # Update weights and biases
    W_k, b_k, W_m, b_m = updateWeights(W_k, b_k, W_m, b_m, dW_k, db_k, dW_m, db_m, alpha)

    # Loss calculation
    loss.append(getLoss(y, y_model))

  # Prediction
  hidden_input = np.dot(X, W_k) + b_k
  hidden_output = relu(hidden_input)
  y_model = np.dot(hidden_output, W_m) + b_m
  return y_model, loss

# TRAINING MODEL

epochs = 100
alpha = 0.000033
y_model, loss = trainModel(X, y, alpha, epochs)

# DATA AND DATA PREDICTION PLOTTING

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X[:,0], X[:,1], y_model, 'b', cmap='cool', label='Model')
ax.scatter(X[:,0], X[:,1], y, 'r', cmap='cool', label='Real')
ax.set_xlabel('Height')
ax.set_ylabel('Weight')
ax.set_zlabel('BMI_model')
ax.legend()
plt.show()

# LOSS PLOTTING

plt.plot(loss)
plt.title(f"Mean-squared error, last: {loss[-1]:.3f}")
plt.xlabel("epochs")
plt.ylabel("loss")
plt.ylim([0, max(loss)*1/10000])
plt.show()